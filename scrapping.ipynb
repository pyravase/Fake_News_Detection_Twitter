{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "#import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preproces(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "def count_by_lambda(expression, word_array):\n",
    "    return len(list(filter(expression, word_array)))\n",
    "def count_occurences(character, word_array):\n",
    "    counter = 0\n",
    "    for j, word in enumerate(word_array):\n",
    "        for char in word:\n",
    "            if char == character:\n",
    "                counter += 1\n",
    "    return counter\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Store the tweets in a dataframe\n",
    "def process_results(results):\n",
    "    id_list = [tweet.id for tweet in results]\n",
    "    data_set = pd.DataFrame(id_list, columns=[\"id\"])\n",
    "        # Processing Tweet Data\n",
    "\n",
    "    data_set[\"text\"] = [tweet.text for tweet in results] #text of tweet\n",
    "    data_set[\"created_at\"] = [tweet.created_at for tweet in results] #when the tweet was created\n",
    "    data_set[\"retweet_count\"] = [tweet.retweet_count for tweet in results] #number of retweets\n",
    "    data_set[\"favorite_count\"] = [tweet.favorite_count for tweet in results] #number of favourites\n",
    "    data_set[\"source\"] = [tweet.source for tweet in results] #source of the tweet\n",
    "    data_set[\"length\"] = [len(tweet.text) for tweet in results] #number of characters in tweet\n",
    "\n",
    "    # Processing User Data\n",
    "    data_set[\"user_id\"] = [tweet.author.id for tweet in results] #id of the author\n",
    "    data_set[\"user_screen_name\"] = [tweet.author.screen_name for tweet in results] \n",
    "    data_set[\"user_name\"] = [tweet.author.name for tweet in results]\n",
    "    data_set[\"user_created_at\"] = [tweet.author.created_at for tweet in results] #age of user account\n",
    "    data_set[\"user_description\"] = [tweet.author.description for tweet in results]\n",
    "    data_set[\"user_followers_count\"] = [tweet.author.followers_count for tweet in results] #number of followers\n",
    "    data_set[\"user_friends_count\"] = [tweet.author.friends_count for tweet in results] #number of friends\n",
    "    data_set[\"user_location\"] = [tweet.author.location for tweet in results] #user has a location in profile?\n",
    "    data_set[\"user_statuses_count\"] = [tweet.author.statuses_count for tweet in results] #number of statuses\n",
    "    data_set[\"user_verified\"] = [tweet.author.verified for tweet in results] #user is verified?\n",
    "    data_set[\"user_url\"] = [tweet.author.url for tweet in results] #user has a URL?\n",
    "    #calculates number of ?, !, hashtags and mentions\n",
    "    data_set['tokenized_text']= data_set['text'].apply(preproces)\n",
    "    data_set['token_texts'] = data_set['tokenized_text'].apply(lambda x : [w for w in x if w.lower() not in stop_words])  \n",
    "    data_set['no_of_question_marks'] = data_set['token_texts'].apply(lambda txt: count_occurences(\"?\", txt)) \n",
    "    data_set['no_of_exclamation_marks'] = data_set['token_texts'].apply(lambda txt: count_occurences(\"!\", txt)) \n",
    "    data_set['no_of_hashtags'] = data_set['token_texts'].apply(lambda txt: count_occurences(\"#\", txt)) \n",
    "    data_set['no_of_mentions'] = data_set['token_texts'].apply(lambda txt: count_occurences(\"@\", txt))\n",
    "    \n",
    "    \n",
    "    #Removes URLs\n",
    "    data_set['cleaned_text'] = data_set['text'].apply(lambda txt:remove_url_by_regex(\"http.?://[^\\s]+[\\s]?\",txt))\n",
    "    #Removes mentions\n",
    "    data_set['cleaned_text'] = data_set['cleaned_text'].apply(lambda txt:remove_url_by_regex(r'(?:@[\\w_]+)',txt))\n",
    "    #Calculates number of colon marks\n",
    "    data_set['no_of_colon_marks'] = data_set['cleaned_text'].apply(lambda txt: count_occurences(\":\", txt)) \n",
    "    data_set['cleaned_text'] = data_set['cleaned_text'].apply(lambda txt:remove_url_by_regex(r'[,|:|\\|=|&|;|%|$|@|^|*|-|#|?|!|.]',txt))\n",
    "    data_set['no_of_words'] = data_set['cleaned_text'].apply(lambda txt:len(re.findall(r'\\w+',txt)))\n",
    "    data_set['no_of_uppercase_words'] = data_set['tokenized_text'].apply(lambda txt: count_by_lambda(lambda word: word == word.upper(),txt))\n",
    "    \n",
    "    \n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hamdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_by_regex(pattern,string):\n",
    "    return re.sub(pattern,\"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping(number,text):\n",
    "  \n",
    "    consumer_key = '2CF0SjF91irHA3xzSRJwhMiJF'\n",
    "    consumer_secret = 'ERNMd3a05M39IZgK1B8C6VeIfFauFMxvfY17LxiTD9omlNha0K'\n",
    "    access_token = '908720407682875393-DkzbECIw8z6Alp3axpLgv3njJ1zcjAU'\n",
    "    access_secret = 'ZMcWhoSNdYFUwT8aItpYrIg95cGJgYD2zfjerxhqfsstu'\n",
    "\n",
    "    \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "    api = tweepy.API(auth)\n",
    "    results = []\n",
    "    i=0\n",
    "    for tweet in tweepy.Cursor(api.search, q=text, lang=\"en\").items():\n",
    "        if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "            results.append(tweet)\n",
    "            id = tweet.id\n",
    "            i=i+1\n",
    "            if i==number:\n",
    "                break\n",
    "    #print (\"we did a scrapping of\"+len(results+\" tweet\"))\n",
    "    df = process_results(results)\n",
    "    df['tokenized_text']= df['text'].apply(preproces)\n",
    "    df[\"user_has_url?\"] = np.where(df[\"user_url\"].isnull(), '0', '1')\n",
    "    \n",
    "    df['user_verified'] = df['user_verified']*1\n",
    "    df['created_at'].fillna('00-00-0000 00:00', inplace=True)\n",
    "    df['user_description'].fillna('Unknown_descr', inplace=True)\n",
    "    df['user_location'].fillna('Unknown_location', inplace=True)\n",
    "    df['user_statuses_count'].fillna('Unknown', inplace=True)\n",
    "    df['user_verified'].fillna('0', inplace=True)\n",
    "    df['user_url'].fillna('Unknown_url', inplace=True)\n",
    "    df[\"created_at\"]=df[\"created_at\"].apply(lambda x: str(x))\n",
    "    df[\"user_created_at\"]=df[\"user_created_at\"].apply(lambda x: str(x))\n",
    "    df['user_has_url?'] = df['user_has_url?'].map( {'1':1, '0':0} )\n",
    "    df[\"user_id\"]=df[\"user_id\"].apply(lambda x: float(x))\n",
    "    df=df[[\"text\",\"created_at\",\"retweet_count\",\"favorite_count\",\"source\",\"length\",\"user_id\",\"user_screen_name\",\"user_name\",\"user_created_at\",\"user_description\",\"user_followers_count\",\"user_friends_count\",\"user_location\",\"user_statuses_count\",\"user_verified\",\"user_url\",\"tokenized_text\",\"token_texts\",\"no_of_question_marks\",\"no_of_exclamation_marks\",\"no_of_hashtags\",\"no_of_mentions\",\"cleaned_text\",\"no_of_colon_marks\",\"no_of_words\",\"no_of_uppercase_words\",\"user_has_url?\"]]\n",
    "\n",
    "    df.dropna(how='any',axis=0,inplace=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=scrapping(50,\"kais said\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
